# ä¸€ä¸ªç»å…¸çš„chainç”±ä¸‰éƒ¨åˆ†æ„æˆï¼šæç¤ºè¯æ¨¡ç‰ˆ + æ¨¡å‹ + è¾“å‡ºè§£æå™¨

from langchain.output_parsers.boolean import BooleanOutputParser
from langchain.prompts import ChatPromptTemplate,PromptTemplate
from langchain.chat_models import init_chat_model
import os
from dotenv import load_dotenv
load_dotenv(override=True)

# åŠ è½½æ¨¡å‹API_KEY
DeepSeek_API_KEY = os.getenv("DEEPSEEK_API_KEY")
model = init_chat_model(model="deepseek-chat",model_provider="deepseek")



# base chain
# prompt_template = ChatPromptTemplate([("system","ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹"),("user","è¿™æ˜¯ç”¨æˆ·çš„é—®é¢˜ï¼š{topic},è¯·ç”¨yesæˆ–noæ¥å›ç­”")])
# bool_qa_chain = prompt_template | model | BooleanOutputParser()

# question = "è¯·é—®1+1æ˜¯å¦å¤§äº2?"
# result = bool_qa_chain.invoke(question)
# print(result)

# base Chain with schemas
from langchain.output_parsers import ResponseSchema,StructuredOutputParser
# schemas = [
#     ResponseSchema(name="age",description="ç”¨æˆ·çš„å¹´é¾„"),
#     ResponseSchema(name="name",description="ç”¨æˆ·åç§°")
# ]

# parser = StructuredOutputParser.from_response_schemas(schemas)
# prompt = PromptTemplate.from_template("è¯·æ ¹æ®ä»¥ä¸‹å†…å®¹æå–ç”¨æˆ·ä¿¡æ¯ï¼Œå¹¶è¿”å›JSONæ ¼å¼:\n{input}\n\m{format_instructions}")
# chain = (prompt.partial(format_instructions=parser.get_format_instructions())  | model | parser)
# result = chain.invoke({"input":"ç”¨æˆ·å«åˆ˜å³°,å»å¹´30å²"})
# print(result)

# complex chain
# news_gen_prompt = PromptTemplate.from_template("è¯·æ ¹æ®ä»¥ä¸‹æ–°é—»æ ‡é¢˜å†™ä¸€ä»½ç®€çŸ­çš„æ–°é—»å†…å®¹(100å­—ä»¥å†…):æ ‡é¢˜:{title}")
# news_chain = news_gen_prompt | model

# schemas = [ResponseSchema(name="time",description="äº‹ä»¶å‘ç”Ÿæ—¶é—´"),ResponseSchema(name="location",description="äº‹ä»¶å‘ç”Ÿçš„åœ°ç‚¹"),ResponseSchema(name="event",description="å‘ç”Ÿçš„å…·ä½“äº‹ä»¶")]
# parser = StructuredOutputParser.from_response_schemas(schemas)
# summary_prompt = PromptTemplate.from_template("è¯·ä»ä¸‹é¢æ–°é—»å†…å®¹ä¸­æå–å…³é”®ä¿¡æ¯,å¹¶è¿”å›JSONæ ¼å¼:{news}{format_instructions}")
# summary_chain = summary_prompt.partial(format_instructions=parser.get_format_instructions()) | model | parser
# print(parser.get_format_instructions())

# from langchain_core.runnables import RunnableLambda
# def debug_print(x):
#     print("ä¸­é—´èŠ‚ç‚¹",x)
#     return x
# debug_node = RunnableLambda(debug_print)
# full_chain = news_chain | debug_node | summary_chain

# result = full_chain.invoke({"title":"è‹±ä¼Ÿè¾¾å…¬å¸åœ¨åŠ å·å‘å¸ƒäº†æ–°çš„GPU"})
# print(result)

# list_memory chain
from langchain_core.messages import AIMessage,HumanMessage,SystemMessage
from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser

parser = StrOutputParser()
prompt = ChatPromptTemplate.from_messages(
    [SystemMessage(content="ä½ å«å°æ™º,æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹"),MessagesPlaceholder(variable_name="messages")])
chain = prompt | model | parser
message_list = []
print("è¾“å…¥ exit æˆ– quit ç»“æŸå¯¹è¯")

while True:
    user_query = input("ä½ : ")
    if user_query.lower() in {"exit","quit"}:
        break
    message_list.append(HumanMessage(content=user_query))
    assistant_reply = chain.invoke({"messages":message_list})
    print("ğŸ¤– å°æ™ºï¼š",assistant_reply)
    message_list.append(AIMessage(content=assistant_reply))
    if len(message_list) > 50:
        message_list = message_list[-50]
